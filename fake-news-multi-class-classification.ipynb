{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def review_to_words(review):\n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"./cache\", \"fake_news_multi-class\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train,data_valid, data_test, labels_train, labels_valid, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        #words_train = list(map(review_to_words, data_train))\n",
    "        #words_test = list(map(review_to_words, data_test))\n",
    "        words_train = [review_to_words(data) for data in data_train]\n",
    "        words_valid = [review_to_words(data) for data in data_valid]\n",
    "        words_test = [review_to_words(data) for data in data_test]\n",
    "        \n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_valid = words_valid, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_valid = labels_valid, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_valid, words_test, labels_train, labels_valid, labels_test = (cache_data['words_train'], cache_data['words_valid'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_valid'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_valid, words_test, labels_train, labels_valid, labels_test\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "# joblib is an enhanced version of pickle that is more efficient for storing NumPy arrays\n",
    "\n",
    "def extract_BoW_features(words_train, words_valid, words_test, vocabulary_size=3200,\n",
    "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Fit a vectorizer to training documents and use it to transform them\n",
    "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "        vectorizer = CountVectorizer(max_features=vocabulary_size,\n",
    "                preprocessor=lambda x: x, tokenizer=lambda x: x)  # already preprocessed\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "        features_valid =  vectorizer.fit_transform(words_valid).toarray()\n",
    "        # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "        features_test = vectorizer.transform(words_test).toarray()\n",
    "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
    "        \n",
    "        # Write to cache file for future runs (store vocabulary as well)\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train=features_train, features_valid=features_valid, features_test=features_test,\n",
    "                             vocabulary=vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                joblib.dump(cache_data, f)\n",
    "            print(\"Wrote features to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        features_train, features_valid, features_test, vocabulary = (cache_data['features_train'], cache_data['features_valid'],\n",
    "                cache_data['features_test'], cache_data['vocabulary'])\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_valid, features_test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapLablesToNumbers(labels):\n",
    "    y_label_dict = {\"pants-fire\" : 0, \"FALSE\" : 1, \"barely-true\" : 2, \"half-true\" : 3, \"mostly-true\" : 4, \"TRUE\" : 5, \"false\": 1, \"true\": 5}\n",
    "    mapped_data = labels.apply(lambda x: y_label_dict[x])\n",
    "    return mapped_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = [\"file\",\"result\",\"news\",\"type\",\"person\",\"desingation\",\"state\",\"party\",\"PF\",\"F\",\"BT\",\"MT\",\"KT\",\"context\"]\n",
    "train_complete_data = pd.read_csv(os.path.join(\"./data\", \"train.tsv\"), sep = \"\\t\", names = columns)\n",
    "valid_complete_data = pd.read_csv(os.path.join(\"./data\", \"valid.tsv\"), sep='\\t', names = columns)\n",
    "test_complete_data = pd.read_csv(os.path.join(\"./data\", \"test.tsv\"), sep='\\t', names = columns)\n",
    "train_data = train_complete_data[\"news\"]\n",
    "train_label = mapLablesToNumbers(train_complete_data[\"result\"])\n",
    "valid_data = valid_complete_data[\"news\"]\n",
    "valid_label = mapLablesToNumbers(valid_complete_data[\"result\"])\n",
    "test_data = test_complete_data[\"news\"]\n",
    "test_label = mapLablesToNumbers(test_complete_data[\"result\"])\n",
    "train_data = pd.concat([train_data,valid_data], axis = 0)\n",
    "train_label = pd.concat([train_label, valid_label], axis = 0)\n",
    "\n",
    "print(len(train_data), \" Train data  - After\")\n",
    "print(len(train_label), \"Train label - After\")\n",
    "print(len(valid_data), \"valid data - After\")\n",
    "print(len(valid_label), \"valid label - After\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data, train_label, valid_label, test_label = preprocess_data(train_data, valid_data, test_data, train_label, valid_label, test_label)\n",
    "train_data, valid_data, test_data, vocabulary = extract_BoW_features(train_data, valid_data, test_data)\n",
    "valid_data = train_data[10240:]\n",
    "train_data = train_data[:10240]\n",
    "valid_label = train_label[10240:]\n",
    "train_label = train_label[:10240]\n",
    "\n",
    "print(len(train_data), \" Train data  - After\")\n",
    "print(len(valid_data), \"valid data - After\")\n",
    "print(len(valid_label), \"valid label - After\")\n",
    "print(len(train_label), \"Train label - After\")\n",
    "\n",
    "train_data, valid_data, test_data = (pd.DataFrame(train_data), pd.DataFrame(valid_data), pd.DataFrame(test_data))\n",
    "train_label, valid_label, test_label = (pd.DataFrame(train_label[:].values), pd.DataFrame(valid_label[:].values), pd.DataFrame(test_label[:].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/xgboost'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "# First, save the test data to test.csv in the data_dir directory. Note that we do not save the associated ground truth\n",
    "# labels, instead we will use them later to compare with our model output.\n",
    "print(len(test_data))\n",
    "pd.DataFrame(test_data).to_csv(os.path.join(data_dir, 'test.csv'), header = False, index = False)\n",
    "# TODO: Save the training and validation data to train.csv and validation.csv in the data_dir directory.\n",
    "#       Make sure that the files you create are in the correct format.\n",
    "\n",
    "# Solution:\n",
    "#\n",
    "#\n",
    "\n",
    "pd.concat([valid_label, valid_data], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header = False, index = False)\n",
    "pd.concat([train_label, train_data], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header = False, index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "\n",
    "# S3 prefix (which folder will we use)\n",
    "prefix = 'fake-multi-class-xgboost'\n",
    "\n",
    "# TODO: Upload the test.csv, train.csv and validation.csv files which are contained in data_dir to S3 using sess.upload_data().\n",
    "test_location = None\n",
    "val_location = None\n",
    "train_location = None\n",
    "\n",
    "# Solution:\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\n",
    "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to retrieve the location of the container which is provided by Amazon for using XGBoost.\n",
    "# As a matter of convenience, the training and inference code both use the same container.\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "container = get_image_uri(session.boto_region_name, 'xgboost','0.90-1')\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a SageMaker estimator using the container location determined in the previous cell.\n",
    "#       It is recommended that you use a single training instance of type ml.m4.xlarge. It is also\n",
    "#       recommended that you use 's3://{}/{}/output'.format(session.default_bucket(), prefix) as the\n",
    "#       output path.\n",
    "\n",
    "xgb = None\n",
    "\n",
    "# Solution:\n",
    "xgb = sagemaker.estimator.Estimator(container, # The location of the container we wish to use\n",
    "                                    role,                                    # What is our current IAM Role\n",
    "                                    train_instance_count=1,                  # How many compute instances\n",
    "                                    train_instance_type='ml.m4.xlarge',      # What kind of compute instances\n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                                    sagemaker_session=session)\n",
    "\n",
    "# TODO: Set the XGBoost hyperparameters in the xgb object. Don't forget that in this case we have a binary\n",
    "#       label so we should be using the 'binary:logistic' objective.\n",
    "\n",
    "# Solution:\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.5,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='multi:softprob',\n",
    "                        early_stopping_rounds=10,\n",
    "                        eval_metric = 'mlogloss',\n",
    "                        num_class = 6,\n",
    "                        num_round=500)\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=val_location, content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure to import the relevant objects used to construct the tuner\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# TODO: Create the hyperparameter tuner object\n",
    "\n",
    "xgb_hyperparameter_tuner = None\n",
    "\n",
    "# Solution:\n",
    "\n",
    "xgb_hyperparameter_tuner = HyperparameterTuner(estimator = xgb, # The estimator object to use as the basis for the training jobs.\n",
    "                                               objective_metric_name = 'validation:mlogloss', # The metric used to compare trained models.\n",
    "                                               objective_type = 'Minimize', # Whether we wish to minimize or maximize the metric.\n",
    "                                               max_jobs = 10, # The total number of models to train\n",
    "                                               max_parallel_jobs = 3, # The number of models to train in parallel\n",
    "                                               hyperparameter_ranges = {\n",
    "                                                    'max_depth': IntegerParameter(4, 8),\n",
    "                                                    'eta'      : ContinuousParameter(0.3, 0.6),\n",
    "                                                    'min_child_weight': IntegerParameter(2, 6),\n",
    "                                                    'subsample': ContinuousParameter(0.5, 0.9),\n",
    "                                                    'gamma': ContinuousParameter(0, 10),\n",
    "                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................................................................................................................................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "xgb_hyperparameter_tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})\n",
    "xgb_hyperparameter_tuner.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-xgboost-200510-0009-010-61510e14'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_hyperparameter_tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-10 00:30:20 Starting - Preparing the instances for training\n",
      "2020-05-10 00:30:20 Downloading - Downloading input data\n",
      "2020-05-10 00:30:20 Training - Training image download completed. Training in progress.\n",
      "2020-05-10 00:30:20 Uploading - Uploading generated training model\n",
      "2020-05-10 00:30:20 Completed - Training job completed\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter eval_metric value mlogloss to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter _tuning_objective_metric value validation:mlogloss to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value multi:softprob to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[00:27:32] 10240x3200 matrix with 32768000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[00:27:33] 1284x3200 matrix with 4108800 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Setting up HPO optimized metric to be : mlogloss\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 10240 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 1284 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-mlogloss:1.77052#011validation-mlogloss:1.77627\u001b[0m\n",
      "\u001b[34m[1]#011train-mlogloss:1.75511#011validation-mlogloss:1.7657\u001b[0m\n",
      "\u001b[34m[2]#011train-mlogloss:1.74332#011validation-mlogloss:1.7591\u001b[0m\n",
      "\u001b[34m[3]#011train-mlogloss:1.73302#011validation-mlogloss:1.7548\u001b[0m\n",
      "\u001b[34m[4]#011train-mlogloss:1.72516#011validation-mlogloss:1.75008\u001b[0m\n",
      "\u001b[34m[5]#011train-mlogloss:1.7186#011validation-mlogloss:1.74805\u001b[0m\n",
      "\u001b[34m[6]#011train-mlogloss:1.71245#011validation-mlogloss:1.74652\u001b[0m\n",
      "\u001b[34m[7]#011train-mlogloss:1.70721#011validation-mlogloss:1.74459\u001b[0m\n",
      "\u001b[34m[8]#011train-mlogloss:1.70293#011validation-mlogloss:1.74417\u001b[0m\n",
      "\u001b[34m[9]#011train-mlogloss:1.69846#011validation-mlogloss:1.74375\u001b[0m\n",
      "\u001b[34m[10]#011train-mlogloss:1.69478#011validation-mlogloss:1.74215\u001b[0m\n",
      "\u001b[34m[11]#011train-mlogloss:1.69251#011validation-mlogloss:1.74214\u001b[0m\n",
      "\u001b[34m[12]#011train-mlogloss:1.68933#011validation-mlogloss:1.74108\u001b[0m\n",
      "\u001b[34m[13]#011train-mlogloss:1.68672#011validation-mlogloss:1.73994\u001b[0m\n",
      "\u001b[34m[14]#011train-mlogloss:1.68509#011validation-mlogloss:1.73997\u001b[0m\n",
      "\u001b[34m[15]#011train-mlogloss:1.68307#011validation-mlogloss:1.74012\u001b[0m\n",
      "\u001b[34m[16]#011train-mlogloss:1.68119#011validation-mlogloss:1.73952\u001b[0m\n",
      "\u001b[34m[17]#011train-mlogloss:1.68042#011validation-mlogloss:1.73932\u001b[0m\n",
      "\u001b[34m[18]#011train-mlogloss:1.67937#011validation-mlogloss:1.73901\u001b[0m\n",
      "\u001b[34m[19]#011train-mlogloss:1.67905#011validation-mlogloss:1.73901\u001b[0m\n",
      "\u001b[34m[20]#011train-mlogloss:1.67782#011validation-mlogloss:1.73777\u001b[0m\n",
      "\u001b[34m[21]#011train-mlogloss:1.67634#011validation-mlogloss:1.73715\u001b[0m\n",
      "\u001b[34m[22]#011train-mlogloss:1.67494#011validation-mlogloss:1.73677\u001b[0m\n",
      "\u001b[34m[23]#011train-mlogloss:1.67425#011validation-mlogloss:1.73637\u001b[0m\n",
      "\u001b[34m[24]#011train-mlogloss:1.67393#011validation-mlogloss:1.73647\u001b[0m\n",
      "\u001b[34m[25]#011train-mlogloss:1.67358#011validation-mlogloss:1.73629\u001b[0m\n",
      "\u001b[34m[26]#011train-mlogloss:1.67328#011validation-mlogloss:1.73617\u001b[0m\n",
      "\u001b[34m[27]#011train-mlogloss:1.6726#011validation-mlogloss:1.7356\u001b[0m\n",
      "\u001b[34m[28]#011train-mlogloss:1.67153#011validation-mlogloss:1.7362\u001b[0m\n",
      "\u001b[34m[29]#011train-mlogloss:1.67119#011validation-mlogloss:1.73643\u001b[0m\n",
      "\u001b[34m[30]#011train-mlogloss:1.671#011validation-mlogloss:1.73601\u001b[0m\n",
      "\u001b[34m[31]#011train-mlogloss:1.66993#011validation-mlogloss:1.73506\u001b[0m\n",
      "\u001b[34m[32]#011train-mlogloss:1.66981#011validation-mlogloss:1.73533\u001b[0m\n",
      "\u001b[34m[33]#011train-mlogloss:1.66924#011validation-mlogloss:1.7351\u001b[0m\n",
      "\u001b[34m[34]#011train-mlogloss:1.66842#011validation-mlogloss:1.73358\u001b[0m\n",
      "\u001b[34m[35]#011train-mlogloss:1.6677#011validation-mlogloss:1.73449\u001b[0m\n",
      "\u001b[34m[36]#011train-mlogloss:1.66718#011validation-mlogloss:1.73436\u001b[0m\n",
      "\u001b[34m[37]#011train-mlogloss:1.66613#011validation-mlogloss:1.73401\u001b[0m\n",
      "\u001b[34m[38]#011train-mlogloss:1.66583#011validation-mlogloss:1.73345\u001b[0m\n",
      "\u001b[34m[39]#011train-mlogloss:1.6654#011validation-mlogloss:1.73278\u001b[0m\n",
      "\u001b[34m[40]#011train-mlogloss:1.66514#011validation-mlogloss:1.73287\u001b[0m\n",
      "\u001b[34m[41]#011train-mlogloss:1.66479#011validation-mlogloss:1.73305\u001b[0m\n",
      "\u001b[34m[42]#011train-mlogloss:1.66435#011validation-mlogloss:1.73296\u001b[0m\n",
      "\u001b[34m[43]#011train-mlogloss:1.66396#011validation-mlogloss:1.73318\u001b[0m\n",
      "\u001b[34m[44]#011train-mlogloss:1.66386#011validation-mlogloss:1.73311\u001b[0m\n",
      "\u001b[34m[45]#011train-mlogloss:1.66294#011validation-mlogloss:1.73256\u001b[0m\n",
      "\u001b[34m[46]#011train-mlogloss:1.66242#011validation-mlogloss:1.73183\u001b[0m\n",
      "\u001b[34m[47]#011train-mlogloss:1.66183#011validation-mlogloss:1.73158\u001b[0m\n",
      "\u001b[34m[48]#011train-mlogloss:1.66161#011validation-mlogloss:1.73164\u001b[0m\n",
      "\u001b[34m[49]#011train-mlogloss:1.66117#011validation-mlogloss:1.73193\u001b[0m\n",
      "\u001b[34m[50]#011train-mlogloss:1.66057#011validation-mlogloss:1.73199\u001b[0m\n",
      "\u001b[34m[51]#011train-mlogloss:1.66049#011validation-mlogloss:1.73191\u001b[0m\n",
      "\u001b[34m[52]#011train-mlogloss:1.6604#011validation-mlogloss:1.73206\u001b[0m\n",
      "\u001b[34m[53]#011train-mlogloss:1.6604#011validation-mlogloss:1.73203\u001b[0m\n",
      "\u001b[34m[54]#011train-mlogloss:1.6604#011validation-mlogloss:1.73214\u001b[0m\n",
      "\u001b[34m[55]#011train-mlogloss:1.6603#011validation-mlogloss:1.73226\u001b[0m\n",
      "\u001b[34m[56]#011train-mlogloss:1.65976#011validation-mlogloss:1.73235\u001b[0m\n",
      "\u001b[34m[57]#011train-mlogloss:1.65976#011validation-mlogloss:1.73232\u001b[0m\n",
      "Training seconds: 223\n",
      "Billable seconds: 223\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a new estimator object attached to the best training job found during hyperparameter tuning\n",
    "\n",
    "xgb_attached = None\n",
    "\n",
    "# Solution:\n",
    "xgb_attached = sagemaker.estimator.Estimator.attach(xgb_hyperparameter_tuner.best_training_job())\n",
    "\n",
    "# TODO: Create a transformer object from the trained model. Using an instance count of 1 and an instance type of ml.m4.xlarge\n",
    "# should be more than enough.\n",
    "xgb_transformer = xgb_attached.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')\n",
    "# TODO: Start the transform job. Make sure to specify the content type and the split type of the test data.\n",
    "xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................\u001b[34m[2020-05-10 03:24:18 +0000] [15] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2020-05-10 03:24:18 +0000] [15] [INFO] Listening at: unix:/tmp/gunicorn.sock (15)\u001b[0m\n",
      "\u001b[34m[2020-05-10 03:24:18 +0000] [15] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2020-05-10 03:24:18 +0000] [22] [INFO] Booting worker with pid: 22\u001b[0m\n",
      "\u001b[34m[2020-05-10 03:24:18 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
      "\u001b[34m[2020-05-10 03:24:18 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[34m[2020-05-10 03:24:19 +0000] [34] [INFO] Booting worker with pid: 34\u001b[0m\n",
      "\n",
      "\u001b[34m[2020-05-10:03:24:49:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [10/May/2020:03:24:49 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2020-05-10:03:24:49:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [10/May/2020:03:24:49 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2020-05-10:03:24:49:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2020-05-10:03:24:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2020-05-10:03:24:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-05-10:03:24:49:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [10/May/2020:03:24:49 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2020-05-10:03:24:49:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [10/May/2020:03:24:49 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2020-05-10:03:24:49:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2020-05-10:03:24:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-05-10:03:24:49:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [10/May/2020:03:24:50 +0000] \"POST /invocations HTTP/1.1\" 200 35572 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [10/May/2020:03:24:50 +0000] \"POST /invocations HTTP/1.1\" 200 35572 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [10/May/2020:03:24:51 +0000] \"POST /invocations HTTP/1.1\" 200 123083 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [10/May/2020:03:24:51 +0000] \"POST /invocations HTTP/1.1\" 200 123083 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2020-05-10T03:24:49.051:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 154.9 KiB/154.9 KiB (1.6 MiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-us-east-1-024597597485/sagemaker-xgboost-200510-0009-010-61510-2020-05-10-03-20-53-874/test.csv.out to data/xgboost/test.csv.out\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1267 test label\n",
      "1267 test data\n",
      "10240 train label\n",
      "10240 train data\n",
      "1284 valid label\n",
      "1284 valid label\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       3\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "5       1\n",
       "6       3\n",
       "7       1\n",
       "8       5\n",
       "9       2\n",
       "10      2\n",
       "11      1\n",
       "12      1\n",
       "13      1\n",
       "14      5\n",
       "15      1\n",
       "16      1\n",
       "17      1\n",
       "18      1\n",
       "19      3\n",
       "20      1\n",
       "21      3\n",
       "22      1\n",
       "23      3\n",
       "24      3\n",
       "25      1\n",
       "26      1\n",
       "27      4\n",
       "28      1\n",
       "29      1\n",
       "       ..\n",
       "1237    1\n",
       "1238    1\n",
       "1239    1\n",
       "1240    4\n",
       "1241    1\n",
       "1242    4\n",
       "1243    1\n",
       "1244    1\n",
       "1245    4\n",
       "1246    1\n",
       "1247    1\n",
       "1248    3\n",
       "1249    1\n",
       "1250    1\n",
       "1251    4\n",
       "1252    1\n",
       "1253    4\n",
       "1254    1\n",
       "1255    1\n",
       "1256    1\n",
       "1257    1\n",
       "1258    1\n",
       "1259    1\n",
       "1260    1\n",
       "1261    1\n",
       "1262    1\n",
       "1263    1\n",
       "1264    1\n",
       "1265    1\n",
       "1266    1\n",
       "Length: 1267, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None)\n",
    "#predictions = [round(num) for num in predictions.squeeze().values]\n",
    "\n",
    "\n",
    "print(len(test_label), 'test label')\n",
    "print(len(test_data), 'test data')\n",
    "print(len(train_label), 'train label')\n",
    "print(len(train_data), 'train data')\n",
    "print(len(valid_label), 'valid label')\n",
    "print(len(valid_data), 'valid label')\n",
    "\n",
    "#tempData = pd.read_csv('./temp/test.csv')\n",
    "#tempData\n",
    "predictions[0] = predictions[0].str.replace('[', \"\")\n",
    "predictions[5] = predictions[5].str.replace(']', \"\")\n",
    "predictions = predictions.apply(pd.to_numeric)\n",
    "predictions = predictions.idxmax(axis=1)\n",
    "\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2067876874506709"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_label, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
